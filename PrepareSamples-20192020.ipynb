{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Notebook: Prepare the training samples from raw data\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The <strong>data treatment</strong>, previous to the network training, it is a very important part when it comes to Machine Learning analyses. There are infinite ways to represent the given information and find the optimal one is often tricky.\n",
    "\n",
    "It is also important to point out that the performance of the architecture will have a huge dependence on how the samples are presented.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Data format\n",
    "\n",
    "\n",
    "### 1.1 Raw Data\n",
    "\n",
    "The events available for the challenge are given in a .csv file with all the information needed for the DNN training. With the exception of the heading, each row corresponds to a single event of the sample.\n",
    "\n",
    "Since we are working with Keras (and Keras is built with NumPy) we will need to transform the rows of the CSV file into NumPy arrays that the DNN can read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading the data\n",
    "\n",
    "The package used to read the data first is [pandas](https://pandas.pydata.org/).\n",
    "\n",
    "By means of the ```read_csv(file, delimiter)``` method we can load all the information of the CSV file into a ```pandas.DataFrame``` object that we can access easily.\n",
    "\n",
    "We can take a look then to the data:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>2.233584</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.347389</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>5.446378</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.245333</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  ...    PRI_jet_num  PRI_jet_leading_pt  \\\n",
       "0               3.064      41.928  ...              2              67.435   \n",
       "1               3.473       2.078  ...              1              46.226   \n",
       "2               3.148       9.336  ...              1              44.251   \n",
       "3               3.310       0.414  ...              0            -999.000   \n",
       "4               3.891      16.405  ...              0            -999.000   \n",
       "\n",
       "   PRI_jet_leading_eta  PRI_jet_leading_phi  PRI_jet_subleading_pt  \\\n",
       "0                2.150                0.444                 46.062   \n",
       "1                0.725                1.158               -999.000   \n",
       "2                2.053               -2.028               -999.000   \n",
       "3             -999.000             -999.000               -999.000   \n",
       "4             -999.000             -999.000               -999.000   \n",
       "\n",
       "   PRI_jet_subleading_eta  PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  \\\n",
       "0                    1.24                  -2.475         113.497  0.002653   \n",
       "1                 -999.00                -999.000          46.226  2.233584   \n",
       "2                 -999.00                -999.000          44.251  2.347389   \n",
       "3                 -999.00                -999.000          -0.000  5.446378   \n",
       "4                 -999.00                -999.000           0.000  6.245333   \n",
       "\n",
       "   Label  \n",
       "0      s  \n",
       "1      b  \n",
       "2      b  \n",
       "3      b  \n",
       "4      b  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "data = pandas.read_csv(\"training.csv\", delimiter = ',') # DataFrame object with al the info of the csv\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And explore a little bit the information we have i.e. number of events, variables... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data: (250000, 33)\n",
      "Number of events: 250000\n",
      "Number of columns: 33\n",
      "\n",
      "List of features in dataset:\n",
      "EventId\n",
      "DER_mass_MMC\n",
      "DER_mass_transverse_met_lep\n",
      "DER_mass_vis\n",
      "DER_pt_h\n",
      "DER_deltaeta_jet_jet\n",
      "DER_mass_jet_jet\n",
      "DER_prodeta_jet_jet\n",
      "DER_deltar_tau_lep\n",
      "DER_pt_tot\n",
      "DER_sum_pt\n",
      "DER_pt_ratio_lep_tau\n",
      "DER_met_phi_centrality\n",
      "DER_lep_eta_centrality\n",
      "PRI_tau_pt\n",
      "PRI_tau_eta\n",
      "PRI_tau_phi\n",
      "PRI_lep_pt\n",
      "PRI_lep_eta\n",
      "PRI_lep_phi\n",
      "PRI_met\n",
      "PRI_met_phi\n",
      "PRI_met_sumet\n",
      "PRI_jet_num\n",
      "PRI_jet_leading_pt\n",
      "PRI_jet_leading_eta\n",
      "PRI_jet_leading_phi\n",
      "PRI_jet_subleading_pt\n",
      "PRI_jet_subleading_eta\n",
      "PRI_jet_subleading_phi\n",
      "PRI_jet_all_pt\n",
      "Weight\n",
      "Label\n"
     ]
    }
   ],
   "source": [
    "### Print general information about the data:\n",
    "print ('Size of data: {}'.format(data.shape))\n",
    "print ('Number of events: {}'.format(data.shape[0]))\n",
    "print ('Number of columns: {}'.format(data.shape[1]))\n",
    "\n",
    "print ('\\nList of features in dataset:')\n",
    "\n",
    "for col in data.columns:\n",
    "    print (col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that ```EventId```, ```Weight``` and ```Label``` are not training features but control variables, because they have no physical information. Therefore, we exclude them in the NumPy array creation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select the features with physical information:\n",
    "features = [h for h in list(data) if h not in ['EventId', 'Weight', 'Label']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note: -999.0 values are missing or meaningless inputs in the data. They do not provide any physical information. For example, in any event with only one jet ```PRI_jet_subleading_pt```, ```PRI_jet_subleading_eta``` or ```PRI_jet_subleading_phi``` would have -999.0 value since there is no jet  where the kinematics can be obtained from.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Physical features representation\n",
    "\n",
    "Before training the network, it is very important to study the physical features we are going to use. This information could give us ideas of how to represent the data or even design the ML algorithm. \n",
    "\n",
    "The distributions of all the physical features are studied. We import the [NumPy](http://www.numpy.org/) package to use ```numpy.array``` structures and also [Matplotlib](https://matplotlib.org/) to generate the histograms.\n",
    "\n",
    "One histogram is created for each physical variable, with signal and background events separately. All -999.0 values are excluded from the plotting since they do not provide any physical nor relevant information. All histograms are saved in the ```Histograms/``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Plotting DER_mass_MMC histogram...\n",
      "    Done\n",
      ">>> Plotting DER_mass_transverse_met_lep histogram...\n",
      "    Done\n",
      ">>> Plotting DER_mass_vis histogram...\n",
      "    Done\n",
      ">>> Plotting DER_pt_h histogram...\n",
      "    Done\n",
      ">>> Plotting DER_deltaeta_jet_jet histogram...\n",
      "    Done\n",
      ">>> Plotting DER_mass_jet_jet histogram...\n",
      "    Done\n",
      ">>> Plotting DER_prodeta_jet_jet histogram...\n",
      "    Done\n",
      ">>> Plotting DER_deltar_tau_lep histogram...\n",
      "    Done\n",
      ">>> Plotting DER_pt_tot histogram...\n",
      "    Done\n",
      ">>> Plotting DER_sum_pt histogram...\n",
      "    Done\n",
      ">>> Plotting DER_pt_ratio_lep_tau histogram...\n",
      "    Done\n",
      ">>> Plotting DER_met_phi_centrality histogram...\n",
      "    Done\n",
      ">>> Plotting DER_lep_eta_centrality histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_tau_pt histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_tau_eta histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_tau_phi histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_lep_pt histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_lep_eta histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_lep_phi histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_met histogram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Done\n",
      ">>> Plotting PRI_met_phi histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_met_sumet histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_num histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_leading_pt histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_leading_eta histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_leading_phi histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_subleading_pt histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_subleading_eta histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_subleading_phi histogram...\n",
      "    Done\n",
      ">>> Plotting PRI_jet_all_pt histogram...\n",
      "    Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Loop over the features\n",
    "for feature in features:\n",
    "    \n",
    "    print(\">>> Plotting \" + feature + \" histogram...\")\n",
    "    \n",
    "    # Signal and background values\n",
    "    signal_values = list( data.loc[data['Label'] == 's', feature] )\n",
    "    background_values = list( data.loc[data['Label'] == 'b', feature] )\n",
    "    \n",
    "    signal_values = list(filter(lambda x: x != -999.0, signal_values))\n",
    "    background_values = list(filter(lambda x: x != -999.0, background_values))\n",
    "    \n",
    "    # Define the histogram binning\n",
    "    xmin = min(signal_values + background_values) \n",
    "    xmax = max(signal_values + background_values)\n",
    "    binning = np.linspace(xmin, xmax, 61) \n",
    "    \n",
    "    # Plot and save the histogram\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize =(6,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.margins(x = 0)\n",
    "    \n",
    "    ax.hist(background_values, bins = binning, color = 'k', alpha = 0.4, histtype = 'stepfilled', \n",
    "             linewidth = 1, edgecolor = 'k', label = 'Background', density = True)\n",
    "    ax.hist(signal_values, bins = binning, color = 'r', alpha = 0.3, histtype = 'stepfilled', \n",
    "             linewidth = 1, edgecolor = 'r', label = 'Signal', density = True)\n",
    "    ax.legend(loc = 'best', fontsize = 10)\n",
    "    ax.set_xlabel(feature, fontsize = 14)\n",
    "    ax.set_ylabel('Event density', fontsize = 14)\n",
    "    ax.tick_params(axis='both', which='both', direction='in', \n",
    "                   bottom=True, top=True, left=True, right=True, labelsize=12)\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0), useMathText = True)\n",
    "    fig.savefig('Histograms/'+feature+'_histo.png', dpi = 600)\n",
    "          \n",
    "    print(\"    Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From raw samples to training datasets\n",
    "\n",
    "In this part of the notebook, it is explained how to get the samples ready for the training. The steps are the following:\n",
    "\n",
    "- **4.1**  Definition of train, development and test datasets\n",
    "\n",
    "- **4.2**  Normalization\n",
    "\n",
    "- **4.3**  NumPy array creation\n",
    "\n",
    "- **4.4**  Class balance\n",
    "\n",
    "- **4.5**  Shuffling\n",
    "\n",
    "- **4.6**  Save variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definition of train and test datasets\n",
    "\n",
    "The first thing to do is select the events that are going to be used for training the DNN (train dataset) and testing its final performance once trained (test dataset).\n",
    "\n",
    "Since both development and test datasets are used to evaluate the DNN performance, it is mandatory to have a proper class balance in them i.e. having the same number of signal and background events. If not, the results obtained from those datasets would not reflect the real DNN efficiency.\n",
    "\n",
    "Let's see how many events of each kind are stored in the csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total signal events: 85667\n",
      "Number of total background events: 164333\n"
     ]
    }
   ],
   "source": [
    "# Get two separate pandas dataframes for signal and background events:\n",
    "signal_data = data.loc[data['Label'] == 's']\n",
    "bkg_data = data.loc[data['Label'] == 'b']\n",
    "\n",
    "print ('Number of total signal events: {}'.format(signal_data.shape[0]))\n",
    "print ('Number of total background events: {}'.format(bkg_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a total of 164333 samples are background events, only 85667 events are hadronic decays of the Higgs boson. This means that we will have only ~34% signal events of the entire dataset for training and testing the DNN.\n",
    "\n",
    "As said above, both development and test datasets must have 50%-50% signal-background balance to obtain reliable results and (for the same reason) no event must be duplicated there. We build those datasets first by taking 5000 events of each kind for each one of them. The remaining events will compoung the train dataset.\n",
    "\n",
    "Each dataset will have associated one ```pandas.DataFrame``` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 5000 events from each class for both the development and test datasets:\n",
    "n_test = 5000\n",
    "\n",
    "signal_test = signal_data[:n_test]\n",
    "bkg_test = bkg_data[:n_test] \n",
    "test = pandas.concat([signal_test, bkg_test]) # final test dataset\n",
    "\n",
    "# The remaining events will compound the training dataframe:\n",
    "\n",
    "signal_train = signal_data[n_test:]\n",
    "bkg_train = bkg_data[n_test:]\n",
    "train = pandas.concat([signal_train, bkg_train]) # final train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normalization\n",
    "\n",
    "Having values of different features taking wildly different ranges could make the learning process harder. A good practice is to do the so-called **feature-wise normalization**, forcing the feature distributions to be centered at 0 while having standard deviations equal to the unity.\n",
    "\n",
    "To have the values properly normalized we could just substract the mean value for each feature, and divide by its standard deviation. So, for each sample <em>i</em> and feature <em>j</em> value $x^{(i)}_j$ we apply:\n",
    "\n",
    "$$ x'^{(i)}_j = \\dfrac{x^{(i)}_j - \\bar{x}_j}{\\sigma (x_j)}$$\n",
    "\n",
    "being the mean value $\\bar{x}_j$ of feature *f* defined as\n",
    "\n",
    "$$\\bar{x}_j = \\dfrac{1}{N}\\sum_{i = 1}^{N} x^{(i)}_j$$\n",
    "\n",
    "and its standard deviation $\\sigma (x_j)$ defined as \n",
    "\n",
    "$$\\sigma(x_j) = \\sqrt{\\dfrac{\\sum_{i = 1}^{N} (x^{(i)}_j - \\bar{x}_j)}{N-1}}$$  \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "> In addition, there are some **IMPORTANT FACTS TO REMEMBER IF THE INPUT IS NORMALIZED** that must be rigurously taken into account:\n",
    "> - Both $\\bar{x}_j$ and $\\sigma (x_j)$ values **must be computed ONLY with the samples of the train dataset**. Remember that development and test datasets are just to measure performance and should not be used for nothing more.\n",
    "> - Although $\\bar{x}_j$ and $\\sigma (x_j)$ are computed with the train dataset, **all the datasets must be normalized in the same way**. The DNN is trained to classify samples of the same kind of those that have been used to compute its weights.\n",
    "> - -999.0 values are meaningless or missing inputs. The DNN is supossed to learn by itself that this value means exactly that, and must be the same for each feature. For these reason, **they must not enter in the $\\bar{x}_j$ and $\\sigma (x_j)$ computations nor be normalized either**. They just have to be left this way.\n",
    "\n",
    "We run over the events of the train dataset and we save the meand and standar deviation values for each one of the features in a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "means = {} # dict with the means of every feature\n",
    "stds = {} # dict with the standard deviations of every feature\n",
    "\n",
    "for feature in features:\n",
    "    true_values = train.loc[data[feature] != -999.0, feature] # We exclude -999.0 values\n",
    "    means[feature] = true_values.values.mean()\n",
    "    stds[feature] = true_values.values.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Input creation\n",
    "\n",
    "The DNN will take plain vectors $\\vec{x}^{(i)}$ as an input. These are given to the network as a matrix of the form\n",
    "\n",
    "$$\\vec{X} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{x}^{(1)} \\\\\n",
    "\\vec{x}^{(2)} \\\\\n",
    "... \\\\\n",
    "\\vec{x}^{(\\text{nEventos})}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "in which each one of the $x^{(i)}$ vectors is identified with an event of the train dataset. Therefore, the columns of this matrix will be asocciated with one feature each. That is \n",
    "\n",
    "$$\\vec{X} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{x}^{(1)}_1 & \\vec{x}^{(1)}_2 & ... & \\vec{x}^{(1)}_{\\text{nFeatures}} \\\\\n",
    "\\vec{x}^{(2)}_1 & \\vec{x}^{(2)}_2 & ... & \\vec{x}^{(2)}_{\\text{nFeatures}} \\\\\n",
    "... & ... & ...& ... \\\\\n",
    "\\vec{x}^{(\\text{nEvents})}_1 & \\vec{x}^{(\\text{nEvents})}_2 & ... &\\vec{x}^{(\\text{nEvents})}_{\\text{nFeatures}} \\\\\n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "  \n",
    "  \n",
    "Appart from the sample matrices $\\vec{X}$ we will need another vector $\\vec{y}$ to let the DNN know if each one of the $\\vec{x}^{(i)}$ vectors corresponds with a signal or a background event. This vector is called label vector and its elements will be either 0 (for background events) or 1 (for signal events):\n",
    "\n",
    "$$\\vec{y} = \n",
    "\\begin{pmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "... \\\\\n",
    "y_{(\\text{nEvents})}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "These two ingredients, sample matrix $\\vec{X}$ an label vector $\\vec{y}$, are the inputs of the network, and we will need to create them for train, develop and test datasets.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Since Keras is build with NumPy, the vectors will need to be defined as ```numpy.array``` objects. To create them, we define first a python function that will receive as an input a ```pandas.DataFrame``` object with the samples, a list with the features we want to include, and two python dictioraries with the mean and standard deviation values (created in the step before). The output of this function will be two ```numpy.array```'s: the sample matrix and the label vector, already normalized.\n",
    "\n",
    "We use this function to create the inputs of the train, development and test dataset.\n",
    "\n",
    "\n",
    "> **Note** that this function contains a loop that runs over all the events in the ```pandas.DataFrame``` given as the input so it will take time to create the vectors. (~1 hour and a half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getXandY(df, fnames, mean_values, std_values):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the sample matrix X and the label vector y constructed with:\n",
    "    - The events of df (pandas.DataFrame)\n",
    "    - The features indicated in fnames (list)\n",
    "    - The mean indicated in mean_values (dictionary)\n",
    "    - The standard deviation indicated in std_values (dictionary)\n",
    "    \"\"\"\n",
    "\n",
    "    progress = [int(i*float(len(df.index.values))) for i in np.linspace(0, 1, 101)]\n",
    "    \n",
    "    X = np.zeros(shape = (df.shape[0], len(fnames))) # empty sample matrix X (to be filled)\n",
    "    Y = np.zeros(shape = (df.shape[0], 1)) # empty label vector y (to be filled)\n",
    "\n",
    "    # Loop over dataset\n",
    "    for i,idx in enumerate(df.index.values):\n",
    "        for j,feature in enumerate(fnames):\n",
    "            \n",
    "            # (i: event)\n",
    "            # (j: feature)\n",
    "        \n",
    "            # X filling:\n",
    "            value = df.loc[idx][feature]\n",
    "            if (value != -999.0): \n",
    "                X[i][j] = (value - mean_values[feature])/std_values[feature]\n",
    "            else: \n",
    "                X[i][j] = -5.0\n",
    "            \n",
    "            # y_test filling:\n",
    "            if df.loc[idx]['Label'] == 's':\n",
    "                Y[i][0] = 1\n",
    "            else:\n",
    "                Y[i][0] = 0\n",
    "                \n",
    "        if (i in progress):\n",
    "            print(\"Progress: \"+ str(i)+\"/\"+str(len(df.index.values)) +\" completed\")\n",
    "                \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/240000 completed\n",
      "Progress: 2400/240000 completed\n",
      "Progress: 4800/240000 completed\n",
      "Progress: 7200/240000 completed\n",
      "Progress: 9600/240000 completed\n",
      "Progress: 12000/240000 completed\n",
      "Progress: 14400/240000 completed\n",
      "Progress: 16800/240000 completed\n",
      "Progress: 19200/240000 completed\n",
      "Progress: 21600/240000 completed\n",
      "Progress: 24000/240000 completed\n",
      "Progress: 26400/240000 completed\n",
      "Progress: 28800/240000 completed\n",
      "Progress: 31200/240000 completed\n",
      "Progress: 33600/240000 completed\n",
      "Progress: 36000/240000 completed\n",
      "Progress: 38400/240000 completed\n",
      "Progress: 40800/240000 completed\n",
      "Progress: 43200/240000 completed\n",
      "Progress: 45600/240000 completed\n",
      "Progress: 48000/240000 completed\n",
      "Progress: 50400/240000 completed\n",
      "Progress: 52800/240000 completed\n",
      "Progress: 55200/240000 completed\n",
      "Progress: 57600/240000 completed\n",
      "Progress: 60000/240000 completed\n",
      "Progress: 62400/240000 completed\n",
      "Progress: 64800/240000 completed\n",
      "Progress: 67200/240000 completed\n",
      "Progress: 69600/240000 completed\n",
      "Progress: 72000/240000 completed\n",
      "Progress: 74400/240000 completed\n",
      "Progress: 76800/240000 completed\n",
      "Progress: 79200/240000 completed\n",
      "Progress: 81600/240000 completed\n",
      "Progress: 84000/240000 completed\n",
      "Progress: 86400/240000 completed\n",
      "Progress: 88800/240000 completed\n",
      "Progress: 91200/240000 completed\n",
      "Progress: 93600/240000 completed\n",
      "Progress: 96000/240000 completed\n",
      "Progress: 98400/240000 completed\n",
      "Progress: 100800/240000 completed\n",
      "Progress: 103200/240000 completed\n",
      "Progress: 105600/240000 completed\n",
      "Progress: 108000/240000 completed\n",
      "Progress: 110400/240000 completed\n",
      "Progress: 112800/240000 completed\n",
      "Progress: 115200/240000 completed\n",
      "Progress: 117600/240000 completed\n",
      "Progress: 120000/240000 completed\n",
      "Progress: 122400/240000 completed\n",
      "Progress: 124800/240000 completed\n",
      "Progress: 127200/240000 completed\n",
      "Progress: 129600/240000 completed\n",
      "Progress: 132000/240000 completed\n",
      "Progress: 134400/240000 completed\n",
      "Progress: 136800/240000 completed\n",
      "Progress: 139200/240000 completed\n",
      "Progress: 141600/240000 completed\n",
      "Progress: 144000/240000 completed\n",
      "Progress: 146400/240000 completed\n",
      "Progress: 148800/240000 completed\n",
      "Progress: 151200/240000 completed\n",
      "Progress: 153600/240000 completed\n",
      "Progress: 156000/240000 completed\n",
      "Progress: 158400/240000 completed\n",
      "Progress: 160800/240000 completed\n",
      "Progress: 163200/240000 completed\n",
      "Progress: 165600/240000 completed\n",
      "Progress: 168000/240000 completed\n",
      "Progress: 170400/240000 completed\n",
      "Progress: 172800/240000 completed\n",
      "Progress: 175200/240000 completed\n",
      "Progress: 177600/240000 completed\n",
      "Progress: 180000/240000 completed\n",
      "Progress: 182400/240000 completed\n",
      "Progress: 184800/240000 completed\n",
      "Progress: 187200/240000 completed\n",
      "Progress: 189600/240000 completed\n",
      "Progress: 192000/240000 completed\n",
      "Progress: 194400/240000 completed\n",
      "Progress: 196800/240000 completed\n",
      "Progress: 199200/240000 completed\n",
      "Progress: 201600/240000 completed\n",
      "Progress: 204000/240000 completed\n",
      "Progress: 206400/240000 completed\n",
      "Progress: 208800/240000 completed\n",
      "Progress: 211200/240000 completed\n",
      "Progress: 213600/240000 completed\n",
      "Progress: 216000/240000 completed\n",
      "Progress: 218400/240000 completed\n",
      "Progress: 220800/240000 completed\n",
      "Progress: 223200/240000 completed\n",
      "Progress: 225600/240000 completed\n",
      "Progress: 228000/240000 completed\n",
      "Progress: 230400/240000 completed\n",
      "Progress: 232800/240000 completed\n",
      "Progress: 235200/240000 completed\n",
      "Progress: 237600/240000 completed\n"
     ]
    }
   ],
   "source": [
    "# Train dataset:\n",
    "x_train, y_train = getXandY(train, features, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/10000 completed\n",
      "Progress: 100/10000 completed\n",
      "Progress: 200/10000 completed\n",
      "Progress: 300/10000 completed\n",
      "Progress: 400/10000 completed\n",
      "Progress: 500/10000 completed\n",
      "Progress: 600/10000 completed\n",
      "Progress: 700/10000 completed\n",
      "Progress: 800/10000 completed\n",
      "Progress: 900/10000 completed\n",
      "Progress: 1000/10000 completed\n",
      "Progress: 1100/10000 completed\n",
      "Progress: 1200/10000 completed\n",
      "Progress: 1300/10000 completed\n",
      "Progress: 1400/10000 completed\n",
      "Progress: 1500/10000 completed\n",
      "Progress: 1600/10000 completed\n",
      "Progress: 1700/10000 completed\n",
      "Progress: 1800/10000 completed\n",
      "Progress: 1900/10000 completed\n",
      "Progress: 2000/10000 completed\n",
      "Progress: 2100/10000 completed\n",
      "Progress: 2200/10000 completed\n",
      "Progress: 2300/10000 completed\n",
      "Progress: 2400/10000 completed\n",
      "Progress: 2500/10000 completed\n",
      "Progress: 2600/10000 completed\n",
      "Progress: 2700/10000 completed\n",
      "Progress: 2800/10000 completed\n",
      "Progress: 2900/10000 completed\n",
      "Progress: 3000/10000 completed\n",
      "Progress: 3100/10000 completed\n",
      "Progress: 3200/10000 completed\n",
      "Progress: 3300/10000 completed\n",
      "Progress: 3400/10000 completed\n",
      "Progress: 3500/10000 completed\n",
      "Progress: 3600/10000 completed\n",
      "Progress: 3700/10000 completed\n",
      "Progress: 3800/10000 completed\n",
      "Progress: 3900/10000 completed\n",
      "Progress: 4000/10000 completed\n",
      "Progress: 4100/10000 completed\n",
      "Progress: 4200/10000 completed\n",
      "Progress: 4300/10000 completed\n",
      "Progress: 4400/10000 completed\n",
      "Progress: 4500/10000 completed\n",
      "Progress: 4600/10000 completed\n",
      "Progress: 4700/10000 completed\n",
      "Progress: 4800/10000 completed\n",
      "Progress: 4900/10000 completed\n",
      "Progress: 5000/10000 completed\n",
      "Progress: 5100/10000 completed\n",
      "Progress: 5200/10000 completed\n",
      "Progress: 5300/10000 completed\n",
      "Progress: 5400/10000 completed\n",
      "Progress: 5500/10000 completed\n",
      "Progress: 5600/10000 completed\n",
      "Progress: 5700/10000 completed\n",
      "Progress: 5800/10000 completed\n",
      "Progress: 5900/10000 completed\n",
      "Progress: 6000/10000 completed\n",
      "Progress: 6100/10000 completed\n",
      "Progress: 6200/10000 completed\n",
      "Progress: 6300/10000 completed\n",
      "Progress: 6400/10000 completed\n",
      "Progress: 6500/10000 completed\n",
      "Progress: 6600/10000 completed\n",
      "Progress: 6700/10000 completed\n",
      "Progress: 6800/10000 completed\n",
      "Progress: 6900/10000 completed\n",
      "Progress: 7000/10000 completed\n",
      "Progress: 7100/10000 completed\n",
      "Progress: 7200/10000 completed\n",
      "Progress: 7300/10000 completed\n",
      "Progress: 7400/10000 completed\n",
      "Progress: 7500/10000 completed\n",
      "Progress: 7600/10000 completed\n",
      "Progress: 7700/10000 completed\n",
      "Progress: 7800/10000 completed\n",
      "Progress: 7900/10000 completed\n",
      "Progress: 8000/10000 completed\n",
      "Progress: 8100/10000 completed\n",
      "Progress: 8200/10000 completed\n",
      "Progress: 8300/10000 completed\n",
      "Progress: 8400/10000 completed\n",
      "Progress: 8500/10000 completed\n",
      "Progress: 8600/10000 completed\n",
      "Progress: 8700/10000 completed\n",
      "Progress: 8800/10000 completed\n",
      "Progress: 8900/10000 completed\n",
      "Progress: 9000/10000 completed\n",
      "Progress: 9100/10000 completed\n",
      "Progress: 9200/10000 completed\n",
      "Progress: 9300/10000 completed\n",
      "Progress: 9400/10000 completed\n",
      "Progress: 9500/10000 completed\n",
      "Progress: 9600/10000 completed\n",
      "Progress: 9700/10000 completed\n",
      "Progress: 9800/10000 completed\n",
      "Progress: 9900/10000 completed\n"
     ]
    }
   ],
   "source": [
    "# Test dataset:\n",
    "x_test, y_test = getXandY(test, features, means, stds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Class balance\n",
    "\n",
    "There is one important thing to take into account when dealing with ML problems, which is the class balance in the datasets. It was mentioned before that development and test datasets must have the same number of signal/background events to give reliable measurements of performance.\n",
    "\n",
    "For example, suppose that a bad trained DNN is very efficient in identifying signal events but classifies background events randomly. If its performance is measured with a dataset with 80% signal events our (mistaken) perception would be that this DNN is correctly trained. \n",
    "\n",
    "And for the same reason a dataset used to measure the DNN performance must not have duplicates.\n",
    "\n",
    "Class balance in development and test datasets is adressed by taken the same number of signal and background events from the available data. But what happen with the train dataset which is used to optimize the DNN that takes the remaining events? \n",
    "\n",
    "Let's see how is the class balance there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class balance in training set:\n",
    "s_train = np.count_nonzero(y_train == 1.0)\n",
    "b_train = np.count_nonzero(y_train == 0.0)\n",
    "print(\"Number of signal events in training set: \" + \"{}\".format(s_train))\n",
    "print(\"Number of background events in training set: \" + \"{}\".format(b_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the number of signal events is much lower than the number of background events. Class unbalance in the train dataset is not as dramatic as in development and test ones but could affect the learning process. It may cause, among other things, that the network only learns how to classify one kind of event. In an extreme scenario, if a train dataset is full of events of one kind, the DNN would learn that the \"optimal\" way to have a good score is to always classify any given event in the most populated category.\n",
    "\n",
    "The best scenario is having a proper 50-50 class balance. There are to valid options to get it:\n",
    "- Change DNN configuration to apply a weight in the less populated class when training\n",
    "- Duplicate some events until matching the most populated class number\n",
    "\n",
    "The last one is the followed one in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_extra = []\n",
    "\n",
    "while(s_train < b_train): # repeat until we have our classes balanced\n",
    "    \n",
    "    for i,idx in enumerate(signal_train.index.values):\n",
    "        \n",
    "        if s_train >= b_train: break # if we have all the extra arrays we need we just quit\n",
    "        \n",
    "        x_extra = np.zeros(len(features))\n",
    "        \n",
    "        for j,feature in enumerate(features):\n",
    "            \n",
    "            value = signal_train.loc[idx][feature]\n",
    "            x_extra[j] = (value - means[feature])/stds[feature]\n",
    "            \n",
    "        x_train_extra.append(x_extra)\n",
    "        s_train += 1\n",
    "        \n",
    "### Add this extra vectors to the existing training dataset:\n",
    "x_train = np.concatenate((x_train, np.array(x_train_extra)), axis = 0)\n",
    "y_train = np.concatenate((y_train, np.full(shape = (len(x_train_extra), 1), fill_value = 1.0)))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we check again the number of events of each class in the train dataset we can see that now it is properly balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print again class balance in training set:\n",
    "s_train = np.count_nonzero(y_train == 1.0)\n",
    "b_train = np.count_nonzero(y_train == 0.0)\n",
    "print(\"Number of signal events in training set: \" + \"{}\".format(s_train))\n",
    "print(\"Number of background events in training set: \" + \"{}\".format(b_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Shuffle the samples\n",
    "\n",
    "Finally, when we have our datasets prepared, it is mandatory to assure that the input in the DNN is correctly shuffled. Events of different classes have to be read by the DNN in an homogeneous way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "### Get the indexes of each dataset:\n",
    "idx_train = np.arange(y_train.shape[0])\n",
    "idx_test = np.arange(y_test.shape[0])\n",
    "\n",
    "### Shuffle them:\n",
    "random.shuffle(idx_train)\n",
    "random.shuffle(idx_test)\n",
    "\n",
    "### And apply the new ordering to every input of each dataset:\n",
    "y_train = y_train[idx_train]\n",
    "x_train = x_train[idx_train]\n",
    "y_test = y_test[idx_test]\n",
    "x_test = x_test[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Save the variables\n",
    "\n",
    "As the sample creation process is long and also computationally expensive, it not advisable to create the inputs every time we want to train the DNN.\n",
    "\n",
    "For this reason, input variables are created once and saved in a pickle file that will allow to access them in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('DNNtraining_variables2.p', 'wb') as file_:\n",
    "        pickle.dump([x_train, y_train, x_test, y_test], file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
