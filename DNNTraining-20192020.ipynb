{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2st Notebook: Sample classification with a DNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the datasets\n",
    "\n",
    "<p> First, we read the NumPy arrays that were generated with the previous Notebook. We count with a pickle file that contains: </p>\n",
    "<ul>\n",
    "    <li> <strong> x_train: </strong> A NumPy array with the samples that are going to train the DNN. </li>\n",
    "    <li> <strong> y_train: </strong> A NumPy array with the labels (0's o 1's) that indicate the class ('signal' or 'background') of the samples contained in <strong>x_train</strong>. </li>\n",
    "    <li> <strong> x_test: </strong> A NumPy array with the samples that are going to evaluate the DNN performance. </li>\n",
    "    <li> <strong> y_test: </strong>  A NumPy array with the labels (0's o 1's) that indicate the class ('signal' or 'background') of the samples contained in <strong>x_test</strong>. This labels will allow to compute the success rate of the DNN.</li>\n",
    "</ul>\n",
    "\n",
    "The NumPy arrays are extracted from the pickle file with the command ```load()```. An example of storing/loading python variables in pickle format is available [here](https://wiki.python.org/moin/UsingPickle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "filename = 'training_variables.p'\n",
    "\n",
    "with open(filename, 'rb') as file_:\n",
    "    x_train, y_train, x_test, y_test = pickle.load(file_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras environment setup\n",
    "\n",
    "To classify the samples with the DNN it is necessary to build the model, then compile it and finally train it. In this practical exercise we work with [keras](https://keras.io/), a module that allows to build the DNN by adding layers and setting activation functions in an easy and handy way.\n",
    "\n",
    "First, we need to import the Keras packages that are necessary for the training, that include, among other things:\n",
    "- ```callbacks``` to access the training process information e.g. loss and accuracy values after each epoch.\n",
    "- Modules to include the DNN layers, activation functions or different implementations to reduce overfitting such as *dropout* or *regularizers*.\n",
    "- Optimizers to define the gradient descent algorithm e.g. ```RMSprop```\n",
    "- The Keras backend over which the DNN is build. In this case we will work with **Tensorflow** (although other options, such as **Theano** are possible).\n",
    "\n",
    "In addition, we also import a module called ```history_tools.py``` which is available in this same repository. It will provide everything necessary to process the callback information when the DNN is training. On one hand, it defines a class ```LossHistory``` that serves to build an instance as the DNN training process evolves. On the other hand, it also imports a function ```plot_history()``` that will receive this instance and plot the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "import keras.callbacks as cb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import time\n",
    "import os \n",
    "\n",
    "from history_tools import LossHistory, plot_history\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DNN creation\n",
    "\n",
    "Before starting the training process, we need to design the architecture of the DNN and then build it. Esentially, we have to decide:\n",
    "- The number of layers of the DNN.\n",
    "- The number of neurons of each layer.\n",
    "- The activation functions of each layer, specially the last one.\n",
    "- The optimizer.\n",
    "- Other functions that could be implemented layer by layer to reduce overfitting e.g. dropout or regularizers.\n",
    "\n",
    "Keras offers two different configurations to build neural networks architectures: The [sequential mode](https://keras.io/getting-started/sequential-model-guide/) and the [functional-api](https://keras.io/getting-started/functional-api-guide/). The later is normally used in problems way more complex that the one treated in this exercise. The DNN that we want to create can be easily build with sequential models, adding layers in a linear way (we encourage you to use this mode).\n",
    "\n",
    "---\n",
    "\n",
    "**TO DO: Define a sequential model and call it ```model```. Then, generate a fully connected network with the following neuron distribution in each one of the hidden layers: ```(30, 20, 20, 10, 10, 10, 5)```. Remember that we are dealing with a binary classification problem: how many neurons must contain the last layer? Remember to include the activation functions recommended in class for each layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTER YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compilation\n",
    "\n",
    "When the DNN model ```model``` is already built i.e having the layer architecture, activation functions... etc correctly defined, the model has to be compiled.\n",
    "\n",
    "In the compilation step we need to decide the **loss function**, **the optimizer**, and also the **metrics** that will be used to monitorize the training process:\n",
    "\n",
    "---\n",
    "> The **loss function** $L(\\hat{y}, y)$ is a value that quantifies the discrepancy between the network predictions and the ground truth i.e. it is a definition of the error in the classification. Each time that the network ends to read a batch of samples and predicts the labels, it computes the loss value and tries to minimize it. The results of the training process depends on how this error value is defined. \n",
    ">\n",
    "> You can find [here](https://keras.io/losses/) some of the functions already defined in Keras. Normally, ```categorical_crossentropy``` is used in multiclass classification problems, while ```binary_crossentropy``` is used in binary ones.\n",
    "---\n",
    "> The **optimizer** is an optimization algorithm that is used to minimize the loss and adjust the weights of the network gradually. Optimizers already defined in Keras can be found [here](https://keras.io/optimizers/). Some examples of them are ```SGD```, ```RMSprop``` or ```Adam```.\n",
    "---\n",
    "> The **metrics** is an auxiliar function that is used to define a value to evaluate the network performance while it is training. The recomended metrics for this problem is the ```accuracy``` value provided by Keras.\n",
    "---\n",
    "---\n",
    "\n",
    "**TO DO: In this part of the Notebook you have to compile your neuronal network. For the first trial use ```binary_crossentropy``` as the loss function and try to minimize it using the ```SGD``` optimizer. Include in your ```metrics``` the accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( ENTER YOUR CODE HERE )\n",
    "print('Model compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Last considerations before training: Sample format\n",
    "\n",
    "At this point the architecture of the network is already defined and compiled. It is now when we have to check the consistency between the format of the inputs and the network we are about to train.\n",
    "\n",
    "Firstly, we have to check that the samples can be read by the DNN. We have to make sure that the network is expecting vectors of a dimension that matches the one of the input i.e. if the samples are 30-dimensional vectors, the DNN should be built to read exactly that.\n",
    "\n",
    "Secondly, we have to keep in mind that the network is going to compare its predictions and the input label vectors (y_train and y_test). So they are expected to have the same format:\n",
    "\n",
    "> If the DNN is a binary network the label need to be given in nEvents x 1 ```numpy.array``` with an unique element per sample that indicates the class:\n",
    ">\n",
    ">```\n",
    "y = numpy.array([[1],\n",
    "                 [0],\n",
    "                 [1],\n",
    "                 [1],\n",
    "                 ...,\n",
    "                 [0]])\n",
    ">```\n",
    "\n",
    "> If, on the contrary, we are dealing with $N$ classes ($N > 2$) the label vector needs a higher dimensional nEvents x $N$ ```numpy.array``` (one element per class). All the entries of each row (row = Event) will be 0 excluding the one that corresponds to the correct class. For example, the last vector ($N = 2$) will be writen as:\n",
    ">```\n",
    ">y = numpy.array([[0, 1],\n",
    "                 [1, 0],\n",
    "                 [0, 1],\n",
    "                 [0, 1],\n",
    "                 ...,\n",
    "                 [1, 0]])\n",
    ">```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural network training \n",
    "\n",
    "Now we need to define the characteristics of the training process i.e. which samples will the DNN use, how many times it will read them, in how many steps, the callbacks... etc.\n",
    "\n",
    "In first place it is recomended to create an instance of the class ```LossHistory```, imported in previous steps. This object is basically a container of **information about the training process**. It stores the accuracy and loss values at the end of every epoch. In order to be updated in this way, it is given as a ```callbacks``` parameter to the DNN.\n",
    "\n",
    "The **number of epochs** also needs to be defined, that is, the number of times that the DNN reads the entire train dataset ```x_train```. It is important to keep in mind that we must reach a compromise between two opposite effects:\n",
    "- If the number of epochs is very small, it is possible that the network does not have enough time to extract the information and learn how to classify the samples properly. In this happens, events would be classified randomly.\n",
    "- If the number of epochs is too big, **overfitting** may happen. Overfitting is the challenge of machine learning and it basically means that *the network has learnt too much*, even the tiny details in which we do not want to base our classification criteria. The DNN could be able to learn systematic effects, fluctuations... that may prevent itself to generalize the model to the test dataset.\n",
    "\n",
    "We also have to define the **batch size** i.e. the number of samples read in each iteration inside an epoch. The network reads the set of samples in the batch, computes the loss, optimizes the weights and reads the next batch until the epoch ends. And it starts again.\n",
    "\n",
    "---\n",
    "\n",
    "The DNN model, defined in the python variable ```model``` is trained by means of the function ```fit``` that takes as main parameters:\n",
    "\n",
    "1. The dataset ```x_train``` and its labels ```y_train``` of the events used to train the network.\n",
    "2. The number of epochs ```epochs```.\n",
    "3. The batch size ```batch_size```.\n",
    "4. The instance of ```LossHistory``` that stores the accuracy and loss values at the end of each epoch, that is given as an argument of ```callbacks```.\n",
    "5. The ```verbose``` level that sets how much infomation about the training do we want to print.\n",
    "6. A boolean value ```shuffle``` that alters the original order of the datasets given as the input. We already shuffled the samples in the previous notebook, but it is advisable to turn this parameter ```True``` anyway. \n",
    "\n",
    "---\n",
    "\n",
    "**TO DO: For now, train the DNN during 20 epochs fixing a ```batch_size``` of 512. Use as validation set a 10% portion of the training dataset and make sure that you are shuffling your samples. Include a ```callbacks``` containing the history of the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print('Training model...')\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "n_epochs = \n",
    "n_batch = \n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model.fit(ENTER YOUR CODE HERE)\n",
    "\n",
    "print(\"Training duration : {0}\".format(time.time() - start_time) + \" secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the training information\n",
    "\n",
    "Once trained, the network weights are fixed and the network can be used to classify other samples that have not been used to train it. Saving the trained model implies saving the architecture (layer number, neurons, dropout... etc) but also **saving the network weights**. In this way, it is possible to use this trained DNN to classify whatever we want without training it again and again.\n",
    "\n",
    "---\n",
    "\n",
    "**TO DO: Save your model in ```.h5``` format in the ```Models/``` folder. Check first if this directory exists and if it does not, create it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTER YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss and accuracy study\n",
    "\n",
    "To check if the training process succeded we have to study the evolution of loss and accuracy. We expect a decrease in the loss value, pointing out that the error in the classification is decreasing as well. On the other hand, we expect an increase in the accurary that would imply that the DNN is becoming more capable of classifying the samples in their correct class.\n",
    "\n",
    "Both values, loss and accuracy, are plotted vs the number of epochs to see the evolution. In most cases we will observe the decrease in the loss and increase in accuracy for the **training set**.\n",
    "\n",
    "However, the behavior of the **validation set** is more complex and will give an idea of how to opmitize the learning process. As the samples of this dataset are different from the ones used to train the network, the performance would be slightly worse.\n",
    "\n",
    "Overfitting manifests itself in the loss and accuracy evolution. When the network starts to learn undesired patterns from the training set that can not be extrapolated to the validation set, the loss stops its decrease and starts to grow up. Consequently, accuracy gets reduced at this point.\n",
    "\n",
    "The function ```plot_history()```, already defined in ```history_tools.py```, can be used to plot both accuracy and loss.\n",
    "\n",
    "---\n",
    "\n",
    "**TO DO: Check if folder ```History/``` exists and if not, create it. Then use ```plot_history()``` to study the loss and accuracy behaviors. Plots will be automatically saved in this folder.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTER YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>TO DO:\n",
    "\n",
    "Now repeat the last steps with the following modifications:\n",
    "\n",
    "- Use the RMSprop optimizer\n",
    "- Use the Adam optimizer\n",
    "\n",
    "What optimizer works better?\n",
    "\n",
    "Using the one that have the best performance: Do you observe something weird in the plot by comparing the accuracy of the train and validation datasets?\n",
    "\n",
    "- Try to decrease the learning rate value\n",
    "\n",
    "What do you observe now?\n",
    "\n",
    "Finally, add to your final model (with the best optimizer/learning rate choice you made) a ```callback``` with ```EarlyStopping``` and ```ModelCheckpoint``` options and train it over 200 epochs. In the ```EarlyStopping``` option use ```val_loss``` value as reference and wait 5 epochs before to stop definitely. In the ```ModelCheckpoint``` use ```val_loss``` as well and save the best model you obtained.\n",
    "\n",
    "(It would be useful to check the callback documentation in Keras [here](https://keras.io/callbacks/))\n",
    "\n",
    "Once you have your best model, you can pass to section 9 of the Notebook.\n",
    "</strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the DNN performance\n",
    "\n",
    "Once you have your best model defined, it is time to evaluate its performance with the samples of the **test dataset** (be aware that this must be the **first time** over this notebook that you use your test dataset). This dataset contains samples that the DNN has not seen before, so the comparison between network predictions and ground truth gives reliable results of the network's discrimination power.\n",
    "\n",
    "The first thing to do is to use the DNN to classify the samples contained in ```x_test``` and obtain the corresponding accuracy and loss values. To this purpose, you have to use the function ```evaluate()``` over the ```model``` object (your best model) once trained. This function takes as parameters the ```x_test``` samples, the ```y_test``` labels and the ```batch_size```.\n",
    "\n",
    "```batch_size``` must be set to 1 in order to classify the samples one by one.\n",
    "\n",
    "---\n",
    "\n",
    "**TO DO: Evaluate your best model by using the test dataset. Call the results of the evaluation ```score```.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTER YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>TO DO: Save the predictions given by the network for each one of the samples of the ```x_test``` dataset in a python variable called ```y_pred```. Print this variable to understand the format in which the network predictions are given.\n",
    "\n",
    "Note: This is the discriminant given by the network.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTER YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Discriminant distribution\n",
    "\n",
    "The discriminant distribution takes a very charactistic shape depending on how much efficient is he DNN classifying  the samples of the test dataset.\n",
    "\n",
    "Those values are plotted for both signal and background events and in this way we can observe how the network predicts the events of each class separately (discriminant values for signal are expected to be close to 1, while background events should be close to 0).\n",
    "\n",
    "We use ```matplotlib.pyplot``` to plot the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Convert the y_pred and y_test numpy matrixes to 1-dimensional numpy arrays:\n",
    "discriminant = np.squeeze(np.asarray(y_pred))\n",
    "true_labels = np.squeeze(np.asarray(y_test))\n",
    "\n",
    "# Get the discriminant values for each class (0: background, 1: signal):\n",
    "discriminant0 = discriminant[list(true_labels == 0)]\n",
    "print(discriminant0)\n",
    "discriminant1 = discriminant[list(true_labels == 1)]\n",
    "print(discriminant1)\n",
    "\n",
    "# Define the binning:\n",
    "binning = np.linspace(0, 1, 51)\n",
    "\n",
    "\n",
    "# Plot the discriminant distributions:\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(6, 6))\n",
    "plt.subplot(111)\n",
    "pdf0, bins0, patches0 = plt.hist(discriminant0, bins = binning, color = 'r', alpha = 0.3, histtype = 'stepfilled', linewidth = 1, edgecolor='r', label = 'Background')\n",
    "pdf1, bins1, patches1 = plt.hist(discriminant1, bins = binning, color = 'b', alpha = 0.3, histtype = 'stepfilled', linewidth = 1, edgecolor='b', label = 'Signal')\n",
    "plt.legend(loc = 'upper center')\n",
    "plt.ylabel('Entries', fontsize = 12)\n",
    "plt.xlabel('DNN discriminant', fontsize = 12)\n",
    "if not os.path.exists('Results/'): os.makedirs('Results/')\n",
    "plt.savefig('Results/Discriminant_distribution.png', dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 ROC curve\n",
    "\n",
    "In a classification problem the main goal is to maximize the number of true classified samples but also to minimize the number of errors. That is, to obtain the higher number of true signal events classified as signal events while avoiding to classify background events as signal events.\n",
    "\n",
    "In this context we have the following definitions:\n",
    "- <font color=\"green\"> True Positives TP: True signal events classified as signal events.</font>\n",
    "- <font color=\"red\"> Fake Positives FP: True background events classified as signal events.</font>\n",
    "- <font color=\"red\"> Fake Negatives FN: True signal events classified as background events.</font>\n",
    "- <font color=\"green\"> True Negatives TN: True background events classified as background events.</font>\n",
    "\n",
    ">The DNN **sensitivity** is defined as the **True Positive Rate (TPR)** i.e. the ratio of True Positives obtained and the total number of true signal samples (true class == 1) in the test set:\n",
    ">$$\\text{TPR} = \\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "> The DNN **False Positive Rate (FPR)** is defined as the ratio between the False Positives obtained and the total number of true background samples (true class == 0) in the test set:\n",
    ">$$\\text{FPR} = \\dfrac{\\text{FP}}{\\text{FP} + \\text{TN}}$$\n",
    "\n",
    "Both TPR and FPR vary as we set the discriminant value over which we interpret that an event is considered as a  signal event. That is, we could say that every event with discriminant > 0.6 is classified as signal or chose another cut value (0.4, 0.5... etc). All this choices have their correspondent TPR and FPR. Our objective is to identify the discriminant cut that gives the highest TPR and lowest FPR relation.\n",
    "\n",
    "The **ROC curve** is defined to study this choice and also evaluate the classification power of our network. It shows the TPR vs FPR relation as the cut in the discriminant varies (from 0 to 1). In Figure 9.1 we show an example of this TPR vs FPR space.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/36/ROC_space-2.png\" width=\"40%\">\n",
    "\n",
    "> **Figure 9.1**: ROC curve example.\n",
    "\n",
    "---\n",
    "\n",
    "The **Area Under the Curve (AUC)** is used to quantify the network performance. The higher it is, the more efficient the network is classifying samples. Normally we have the following criteria to evaluate the AUC value:\n",
    "\n",
    "```.90-1 = excellent (A)```\n",
    "```.80-.90 = good (B)```\n",
    "```.70-.80 = fair (C)```\n",
    "```.60-.70 = poor (D)```\n",
    "```.50-.60 = fail (F)```\n",
    "\n",
    "In order to implement the **ROC curve** creation in the code we use we use the python package ```sklearn``` (see [here](https://scikit-learn.org/stable/)). Inside the module ```sklearn.metrics``` there is a function ```roc_curve()``` that takes the network predictions ```y_pred``` and the ground truth ```y_test``` for the samples of the **test dataset**. The function computes the **TPR** (```tpr```) and the **FPR** (```fpr```) by scanning the possible values of the discriminant cut (```thresholds```) and returns that information contained in ```numpy.array``` objects. An example of the code implementation would be something like:\n",
    "\n",
    "```fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)```\n",
    "\n",
    "**AUC** can be easily computed from ```fpr``` and ```tpr``` by using another function provided by ```sklearn```:\n",
    "\n",
    "```auc = metrics.auc(fpr, tpr)```\n",
    "\n",
    "**TO DO: The full implementation is already done and you can check the results of your DNN by just running:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(6, 6))\n",
    "plt.subplot(111)\n",
    "plt.plot(fpr, tpr, color = 'r', label = \"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label = \"Random guess\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xlabel('False Positive rate', fontsize = 12)\n",
    "plt.ylabel('True Positive rate', fontsize = 12)\n",
    "plt.text(0.68, 0.1, 'AUC: %.3f' % auc)\n",
    "plt.savefig('Results/ROC.png', dpi =  600)\n",
    "\n",
    "\n",
    "#### Optative and not explained (Youden index, cut value):\n",
    "Youden_index = tpr - fpr\n",
    "i_max = np.argmax(Youden_index)\n",
    "cut_value = thresholds[i_max]\n",
    "\n",
    "print(\"The optimal cut value is: \" + str(cut_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Confusion matrix\n",
    "\n",
    "Finally and with the optimal cut value in the discriminant already chosen, the results of the DNN performance can be presented in the form of a **confusion matrix**.\n",
    "\n",
    ">Being $N$ the number of classes, the **confusion matrix** is a $N$x$N$ matrix where the rows are the true classes of the samples and the columns the class predictions given by the network. In this case the confusion matrix is a 2x2 matrix with two classes (0: background, 1: signal). The entries of the matrix are the fraction of samples of the test dataset filled in each category.\n",
    "\n",
    "In this sense, the confusion matrix can be understood as a 2-dimensional normalized histogram.\n",
    "\n",
    "According to this definition, the **True Positives (TP)** are stored in (1,1), **True Negatives (TN)** in (0,0), **False Positives (FP)** in (0,1) and **False Negatives (FN)** in (1,0).\n",
    "\n",
    "**TO DO: The confusion matrix is already implemented and you can obtain the one from your model by running:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from results_tools import plot_confusion_matrix\n",
    "\n",
    "print(\"-> Computing confusion matrix...\")\n",
    "\n",
    "y_test_confusion = true_labels\n",
    "y_pred_confusion = np.zeros(len(discriminant))\n",
    "\n",
    "for i in range(0, len(discriminant)):\n",
    "    if discriminant[i] > cut_value: # threshold\n",
    "        y_pred_confusion[i] = 1\n",
    "    else:\n",
    "        y_pred_confusion[i] = 0\n",
    "        \n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test_confusion, y_pred_confusion)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm=cnf_matrix, classes=['Background (0)', 'Signal (1)'], normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "if not os.path.exists('Results/'): os.makedirs('Results/')\n",
    "plt.savefig(\"Results/ConfusionMatrix.png\", dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
